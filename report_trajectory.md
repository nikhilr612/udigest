# Trajectory: Paper 1 / 105

Decision:  Relevant | Steps: 5

1. The paper introduces mHC, a manifold-constrained extension of Hyper-Connections aimed at restoring identity mapping and improving scalability. Given my interest in residual connections and theoretical deep-learning advances, I should first survey the most recent literature on “Hyper-Connections” to see how mHC compares with concurrent work and identify any follow-up studies or critiques published after this arXiv pre-print.
   Tool: `arxiv_fetch_most_recent`

   ```python
   {'query': 'Hyper-Connections manifold constrained residual identity mapping', 'k': 10}
   ```

   ```
   ['Title: One-step Latent-free Image Generation with Pixel Mean Flows\nLink: http://arxiv.org/abs/2601.22158v1\nSummary:\nModern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in a latent space. Recent advances have made encouraging progress on each aspect individually, paving the way toward one-step diffusion/flow without latents. In this work, we take a further step towards this goal and propose "pixel MeanFlow" (pMF). Our core guideline is to formulate the network output space and the loss space separately. The network target is designed to be on a presumed low-dimensional image manifold (i.e., x-prediction), while the loss is defined via MeanFlow in the velocity space. We introduce a simple transformation between the image manifold and the average velocity field. In experiments, pMF achieves strong results for one-step latent-free generation on ImageNet at 256x256 resolution (2.22 FID) and 512x512 resolution (2.48 FID), filling a key missing piece in this regime. We hope that our study will further advance the boundaries of diffusion/flow-based generative models.\nCategories: cs.CV\nPublished On: 2026-01-29', "Title: Cobordism and Concordance of Surfaces in 4-Manifolds\nLink: http://arxiv.org/abs/2601.22152v1\nSummary:\nWe show that two properly embedded compact surfaces in an orientable 4-manifold are cobordant if and only if they are $\\mathbb{Z}/2$-homologous and either the 4-manifold has boundary or the surfaces have the same normal Euler number. If the 4-manifold is simply-connected and the surfaces are closed, non-orientable, and cobordant, we show that they are in fact concordant. This completes the classification of closed surfaces in simply-connected 4-manifolds up to concordance. Our methods give new constructions of cobordisms with prescribed boundaries, and completely determine when a given cobordism between the boundaries extends to a cobordism or concordance between the surfaces. We obtain our concordance results by extending Sunukjian's method of ambient surgery to the unoriented case using Pin$^-$-structures. We also discuss conditions for an arbitrary codimension 2 properly embedded submanifold to admit an unoriented spanning manifold with prescribed boundary. All results hold in both the smooth and topological categories.\nCategories: math.GT\nPublished On: 2026-01-29", 'Title: Late Breaking Results: Conversion of Neural Networks into Logic Flows for Edge Computing\nLink: http://arxiv.org/abs/2601.22151v1\nSummary:\nNeural networks have been successfully applied in various resource-constrained edge devices, where usually central processing units (CPUs) instead of graphics processing units exist due to limited power availability. State-of-the-art research still focuses on efficiently executing enormous numbers of multiply-accumulate (MAC) operations. However, CPUs themselves are not good at executing such mathematical operations on a large scale, since they are more suited to execute control flow logic, i.e., computer algorithms. To enhance the computation efficiency of neural networks on CPUs, in this paper, we propose to convert them into logic flows for execution. Specifically, neural networks are first converted into equivalent decision trees, from which decision paths with constant leaves are then selected and compressed into logic flows. Such logic flows consist of if and else structures and a reduced number of MAC operations. Experimental results demonstrate that the latency can be reduced by up to 14.9 % on a simulated RISC-V CPU without any accuracy degradation.\n  The code is open source at https://github.com/TUDa-HWAI/NN2Logic\nCategories: cs.LG, eess.SY\nPublished On: 2026-01-29', 'Title: JUST-DUB-IT: Video Dubbing via Joint Audio-Visual Diffusion\nLink: http://arxiv.org/abs/2601.22143v1\nSummary:\nAudio-Visual Foundation Models, which are pretrained to jointly generate sound and visual content, have recently shown an unprecedented ability to model multi-modal generation and editing, opening new opportunities for downstream tasks. Among these tasks, video dubbing could greatly benefit from such priors, yet most existing solutions still rely on complex, task-specific pipelines that struggle in real-world settings. In this work, we introduce a single-model approach that adapts a foundational audio-video diffusion model for video-to-video dubbing via a lightweight LoRA. The LoRA enables the model to condition on an input audio-video while jointly generating translated audio and synchronized facial motion. To train this LoRA, we leverage the generative model itself to synthesize paired multilingual videos of the same speaker. Specifically, we generate multilingual videos with language switches within a single clip, and then inpaint the face and audio in each half to match the language of the other half. By leveraging the rich generative prior of the audio-visual model, our approach preserves speaker identity and lip synchronization while remaining robust to complex motion and real-world dynamics. We demonstrate that our approach produces high-quality dubbed videos with improved visual fidelity, lip synchronization, and robustness compared to existing dubbing pipelines.\nCategories: cs.GR, cs.CV\nPublished On: 2026-01-29', 'Title: SMOG: Scalable Meta-Learning for Multi-Objective Bayesian Optimization\nLink: http://arxiv.org/abs/2601.22131v1\nSummary:\nMulti-objective optimization aims to solve problems with competing objectives, often with only black-box access to a problem and a limited budget of measurements. In many applications, historical data from related optimization tasks is available, creating an opportunity for meta-learning to accelerate the optimization. Bayesian optimization, as a promising technique for black-box optimization, has been extended to meta-learning and multi-objective optimization independently, but methods that simultaneously address both settings - meta-learned priors for multi-objective Bayesian optimization - remain largely unexplored. We propose SMOG, a scalable and modular meta-learning model based on a multi-output Gaussian process that explicitly learns correlations between objectives. SMOG builds a structured joint Gaussian process prior across meta- and target tasks and, after conditioning on metadata, yields a closed-form target-task prior augmented by a flexible residual multi-output kernel. This construction propagates metadata uncertainty into the target surrogate in a principled way. SMOG supports hierarchical, parallel training: meta-task Gaussian processes are fit once and then cached, achieving linear scaling with the number of meta-tasks. The resulting surrogate integrates seamlessly with standard multi-objective Bayesian optimization acquisition functions.\nCategories: cs.LG\nPublished On: 2026-01-29', 'Title: World of Workflows: a Benchmark for Bringing World Models to Enterprise Systems\nLink: http://arxiv.org/abs/2601.22130v1\nSummary:\nFrontier large language models (LLMs) excel as autonomous agents in many domains, yet they remain untested in complex enterprise systems where hidden workflows create cascading effects across interconnected databases. Existing enterprise benchmarks evaluate surface-level agentic task completion similar to general consumer benchmarks, ignoring true challenges in enterprises, such as limited observability, large database state, and hidden workflows with cascading side effects. We introduce World of Workflows (WoW), a realistic ServiceNow-based environment incorporating 4,000+ business rules and 55 active workflows embedded in the system, alongside WoW-bench, a benchmark of 234 tasks evaluating constrained agentic task completion and enterprise dynamics modeling capabilities. We reveal two major takeaways: (1) Frontier LLMs suffer from dynamics blindness, consistently failing to predict the invisible, cascading side effects of their actions, which leads to silent constraint violations, and (2) reliability in opaque systems requires grounded world modeling, where agents must mentally simulate hidden state transitions to bridge the observability gap when high-fidelity feedback is unavailable. For reliable and useful enterprise agents, WoW motivates a new paradigm to explicitly learn system dynamics. We release our GitHub for setting up and evaluating WoW.\nCategories: cs.AI, cs.SE\nPublished On: 2026-01-29', 'Title: EditYourself: Audio-Driven Generation and Manipulation of Talking Head Videos with Diffusion Transformers\nLink: http://arxiv.org/abs/2601.22127v1\nSummary:\nCurrent generative video models excel at producing novel content from text and image prompts, but leave a critical gap in editing existing pre-recorded videos, where minor alterations to the spoken script require preserving motion, temporal coherence, speaker identity, and accurate lip synchronization. We introduce EditYourself, a DiT-based framework for audio-driven video-to-video (V2V) editing that enables transcript-based modification of talking head videos, including the seamless addition, removal, and retiming of visually spoken content. Building on a general-purpose video diffusion model, EditYourself augments its V2V capabilities with audio conditioning and region-aware, edit-focused training extensions. This enables precise lip synchronization and temporally coherent restructuring of existing performances via spatiotemporal inpainting, including the synthesis of realistic human motion in newly added segments, while maintaining visual fidelity and identity consistency over long durations. This work represents a foundational step toward generative video models as practical tools for professional video post-production.\nCategories: cs.CV, cs.GR, cs.LG, cs.MM\nPublished On: 2026-01-29', 'Title: An Invitation to Higher-Order Riemannian Optimization: Optimal and Implementable Methods\nLink: http://arxiv.org/abs/2601.22126v1\nSummary:\nThis paper presents the first optimal-rate $p$-th order methods with $p\\geq 1$ for finding first and second-order stationary points of non-convex smooth objective functions over Riemannian manifolds. In contrast to the geodesically convex setting, we definitively establish that the optimal oracle complexity of non-convex optimization over manifolds matches that over Euclidean space. In parallel with the complexity analysis, we introduce a general framework for systematically studying higher-order regularity on Riemannian manifolds that characterizes its joint dependence on the objective function and the chosen retraction. To the best of our knowledge, this framework constitutes the first known application in optimization of pullback connections and the Sasaki metric to the study of retraction-based pullbacks of the objective function. We provide clean derivative bounds based on a new covariant Faà di Bruno formula derived within our framework. For $p=3$, our methods are fully implementable via a new Krylov-based framework for minimizing quartically regularized cubic polynomials. This is the first Krylov method for this class of polynomials and may be of independent interest beyond Riemannian optimization.\nCategories: math.OC\nPublished On: 2026-01-29', 'Title: Learning Hamiltonian Flow Maps: Mean Flow Consistency for Large-Timestep Molecular Dynamics\nLink: http://arxiv.org/abs/2601.22123v1\nSummary:\nSimulating the long-time evolution of Hamiltonian systems is limited by the small timesteps required for stable numerical integration. To overcome this constraint, we introduce a framework to learn Hamiltonian Flow Maps by predicting the mean phase-space evolution over a chosen time span $Δt$, enabling stable large-timestep updates far beyond the stability limits of classical integrators. To this end, we impose a Mean Flow consistency condition for time-averaged Hamiltonian dynamics. Unlike prior approaches, this allows training on independent phase-space samples without access to future states, avoiding expensive trajectory generation. Validated across diverse Hamiltonian systems, our method in particular improves upon molecular dynamics simulations using machine-learned force fields (MLFF). Our models maintain comparable training and inference cost, but support significantly larger integration timesteps while trained directly on widely-available trajectory-free MLFF datasets.\nCategories: cs.LG\nPublished On: 2026-01-29', "Title: Microlocal maximal hypoellipticity from the geometric viewpoint: I\nLink: http://arxiv.org/abs/2601.22122v1\nSummary:\nGiven some vector fields on a smooth manifold satisfying Hörmander's condition, we define a bi-graded pseudo-differential calculus which contains the classical pseudo-differential calculus and a pseudo-differential calculus adapted to the sub-Riemannian structure induced by the vector fields.\n  Our approach is based on geometric constructions (resolution of singularities) together with methods from operators algebras. We develop this calculus in full generality, including Sobolev spaces, the wavefront set, and the principal symbol, etc.\n  In particular, using this calculus, we prove that invertibility of the principal symbol implies microlocal maximal hypoellipticity. This allows us to resolve affirmatively the microlocal version of a conjecture of Helffer and Nourrigat.\nCategories: math.AP, math.OA\nPublished On: 2026-01-29"]
   ```

1. The previous search returned very recent papers but none directly on “Hyper-Connections” or mHC.  I need to cast a wider net to find the original HC paper and any immediate follow-ups so I can compare mHC against its precursor and see whether the manifold constraint idea has been picked up by others.  Fetching the most relevant papers rather than the most recent should give better coverage of the lineage.
   Tool: `arxiv_fetch_most_relevant`

   ```python
   {'query': 'Hyper-Connections residual connection identity mapping', 'k': 10}
   ```

   ```
   ['Title: RMNet: Equivalently Removing Residual Connection from Networks\nLink: http://arxiv.org/abs/2111.00687v1\nSummary:\nAlthough residual connection enables training very deep neural networks, it is not friendly for online inference due to its multi-branch topology. This encourages many researchers to work on designing DNNs without residual connections at inference. For example, RepVGG re-parameterizes multi-branch topology to a VGG-like (single-branch) model when deploying, showing great performance when the network is relatively shallow. However, RepVGG can not transform ResNet to VGG equivalently because re-parameterizing methods can only be applied to linear blocks and the non-linear layers (ReLU) have to be put outside of the residual connection which results in limited representation ability, especially for deeper networks. In this paper, we aim to remedy this problem and propose to remove the residual connection in a vanilla ResNet equivalently by a reserving and merging (RM) operation on ResBlock. Specifically, the RM operation allows input feature maps to pass through the block while reserving their information and merges all the information at the end of each block, which can remove residual connections without changing the original output. As a plug-in method, RM Operation basically has three advantages: 1) its implementation makes it naturally friendly for high ratio network pruning. 2) it helps break the depth limitation of RepVGG. 3) it leads to better accuracy-speed trade-off network (RMNet) compared to ResNet and RepVGG. We believe the ideology of RM Operation can inspire many insights on model design for the community in the future. Code is available at: https://github.com/fxmeng/RMNet.\nCategories: cs.CV, cs.LG\nPublished On: 2021-11-01', 'Title: Hyper-Connections\nLink: http://arxiv.org/abs/2409.19606v3\nSummary:\nWe present hyper-connections, a simple yet effective method that can serve as an alternative to residual connections. This approach specifically addresses common drawbacks observed in residual connection variants, such as the seesaw effect between gradient vanishing and representation collapse. Theoretically, hyper-connections allow the network to adjust the strength of connections between features at different depths and dynamically rearrange layers. We conduct experiments focusing on the pre-training of large language models, including dense and sparse models, where hyper-connections show significant performance improvements over residual connections. Additional experiments conducted on vision tasks also demonstrate similar improvements. We anticipate that this method will be broadly applicable and beneficial across a wide range of AI problems.\nCategories: cs.LG, cs.CL, cs.CV, cs.NE\nPublished On: 2024-09-29', 'Title: mHC: Manifold-Constrained Hyper-Connections\nLink: http://arxiv.org/abs/2512.24880v2\nSummary:\nRecently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns. While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead. To address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Empirical experiments demonstrate that mHC is effective for training at scale, offering tangible performance improvements and superior scalability. We anticipate that mHC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models.\nCategories: cs.CL, cs.AI, cs.LG\nPublished On: 2025-12-31', 'Title: KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices\nLink: http://arxiv.org/abs/2601.21579v1\nSummary:\nThe success of Hyper-Connections (HC) in neural networks (NN) has also highlighted issues related to its training instability and restricted scalability. The Manifold-Constrained Hyper-Connections (mHC) mitigate these challenges by projecting the residual connection space onto a Birkhoff polytope, however, it faces two issues: 1) its iterative Sinkhorn-Knopp (SK) algorithm does not always yield exact doubly stochastic residual matrices; 2) mHC incurs a prohibitive $\\mathcal{O}(n^3C)$ parameter complexity with $n$ as the width of the residual stream and $C$ as the feature dimension. The recently proposed mHC-lite reparametrizes the residual matrix via the Birkhoff-von-Neumann theorem to guarantee double stochasticity, but also faces a factorial explosion in its parameter complexity, $\\mathcal{O} \\left( nC \\cdot n! \\right)$. To address both challenges, we propose \\textbf{KromHC}, which uses the \\underline{Kro}necker products of smaller doubly stochastic matrices to parametrize the residual matrix in \\underline{mHC}. By enforcing manifold constraints across the factor residual matrices along each mode of the tensorized residual stream, KromHC guarantees exact double stochasticity of the residual matrices while reducing parameter complexity to $\\mathcal{O}(n^2C)$. Comprehensive experiments demonstrate that KromHC matches or even outperforms state-of-the-art (SOTA) mHC variants, while requiring significantly fewer trainable parameters. The code is available at \\texttt{https://github.com/wz1119/KromHC}.\nCategories: cs.CL, cs.LG\nPublished On: 2026-01-29', 'Title: Frac-Connections: Fractional Extension of Hyper-Connections\nLink: http://arxiv.org/abs/2503.14125v1\nSummary:\nResidual connections are central to modern deep learning architectures, enabling the training of very deep networks by mitigating gradient vanishing. Hyper-Connections recently generalized residual connections by introducing multiple connection strengths at different depths, thereby addressing the seesaw effect between gradient vanishing and representation collapse. However, Hyper-Connections increase memory access costs by expanding the width of hidden states. In this paper, we propose Frac-Connections, a novel approach that divides hidden states into multiple parts rather than expanding their width. Frac-Connections retain partial benefits of Hyper-Connections while reducing memory consumption. To validate their effectiveness, we conduct large-scale experiments on language tasks, with the largest being a 7B MoE model trained on up to 3T tokens, demonstrating that Frac-Connections significantly outperform residual connections.\nCategories: cs.LG, cs.AI, cs.CL\nPublished On: 2025-03-18', 'Title: Hyper-holomorphic connections on vector bundles on hyper-Kähler manifolds\nLink: http://arxiv.org/abs/2204.04075v2\nSummary:\nWe study infinitesimal deformations of autodual and hyper-holomorphic connections on complex vector bundles on hyper-Kähler manifolds of arbitrary dimension. In particular, we describe the DG Lie algebra controlling this deformation problem. Moreover, we prove associative formality for derived endomorphisms of a holomorphic vector bundle admitting a projectively hyper-holomorphic connection.\nCategories: math.AG\nPublished On: 2022-04-08', 'Title: Identity Connections in Residual Nets Improve Noise Stability\nLink: http://arxiv.org/abs/1905.10944v1\nSummary:\nResidual Neural Networks (ResNets) achieve state-of-the-art performance in many computer vision problems. Compared to plain networks without residual connections (PlnNets), ResNets train faster, generalize better, and suffer less from the so-called degradation problem. We introduce simplified (but still nonlinear) versions of ResNets and PlnNets for which these discrepancies still hold, although to a lesser degree. We establish a 1-1 mapping between simplified ResNets and simplified PlnNets, and show that they are exactly equivalent to each other in expressive power for the same computational complexity. We conjecture that ResNets generalize better because they have better noise stability, and empirically support it for both simplified and fully-fledged networks.\nCategories: cs.LG, cs.CV, stat.ML\nPublished On: 2019-05-27', 'Title: Identity Mappings in Deep Residual Networks\nLink: http://arxiv.org/abs/1603.05027v3\nSummary:\nDeep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers\nCategories: cs.CV, cs.LG\nPublished On: 2016-03-16', 'Title: Learning Identity Mappings with Residual Gates\nLink: http://arxiv.org/abs/1611.01260v2\nSummary:\nWe propose a new layer design by adding a linear gating mechanism to shortcut connections. By using a scalar parameter to control each gate, we provide a way to learn identity mappings by optimizing only one parameter. We build upon the motivation behind Residual Networks, where a layer is reformulated in order to make learning identity mappings less problematic to the optimizer. The augmentation introduces only one extra parameter per layer, and provides easier optimization by making degeneration into identity mappings simpler. We propose a new model, the Gated Residual Network, which is the result when augmenting Residual Networks. Experimental results show that augmenting layers provides better optimization, increased performance, and more layer independence. We evaluate our method on MNIST using fully-connected networks, showing empirical indications that our augmentation facilitates the optimization of deep models, and that it provides high tolerance to full layer removal: the model retains over 90% of its performance even after half of its layers have been randomly removed. We also evaluate our model on CIFAR-10 and CIFAR-100 using Wide Gated ResNets, achieving 3.65% and 18.27% error, respectively.\nCategories: cs.CV, cs.LG\nPublished On: 2016-11-04', "Title: Set-Theoretically Perfect Ideals and Residual Intersections\nLink: http://arxiv.org/abs/2409.05705v2\nSummary:\nThis paper studies algebraic residual intersections in rings with Serre's condition \\( S_{s} \\). It demonstrates that residual intersections admit free approaches i.e. perfect subideal with the same radical. This fact leads to determining a uniform upper bound for the multiplicity of residual intersections. In positive characteristic, it follows that residual intersections are cohomologically complete intersection and, hence, their variety is connected in codimension one.\nCategories: math.AC\nPublished On: 2024-09-09"]
   ```

1. Now I have the original HC paper (2409.19606) and two direct descendants: mHC itself and KromHC, plus Frac-Connections.  To situate the manifold-constrained idea in a broader theoretical context I should look up the Birkhoff–von-Neumann theorem and the geometry of doubly-stochastic matrices, since mHC projects residual matrices onto that manifold.  A quick Wikipedia search will give me the mathematical background needed to comment on why this particular manifold restores identity mapping and whether similar geometric constraints appear elsewhere in deep-learning theory.
   Tool: `wikipedia_term_search`

   ```python
   {'term': 'Birkhoff polytope doubly stochastic matrix', 'k': 3}
   ```

   ```
   ['In mathematics, especially in probability and combinatorics, a doubly stochastic matrix \n(also called bistochastic matrix) is a square matrix \n  \n    \n      \n        X\n        =\n        (\n        \n          x\n          \n            i\n            j\n          \n        \n        )\n      \n    \n    {\\displaystyle X=(x_{ij})}\n  \n of nonnegative real numbers, each of whose rows and columns sums to 1, i.e.,\n\n  \n    \n      \n        \n          ∑\n          \n            i\n          \n        \n        \n          x\n          \n            i\n            j\n          \n        \n        =\n        \n          ∑\n          \n            j\n          \n        \n        \n          x\n          \n            i\n            j\n          \n        \n        =\n        1\n        ,\n      \n    \n    {\\displaystyle \\sum _{i}x_{ij}=\\sum _{j}x_{ij}=1,}\n  \n\nThus, a doubly stochastic matrix is both left stochastic and right stochastic.\nIndeed, any matrix that is both left and right stochastic must be square: if every row sums to 1 then the sum of all entries in the matrix must be equal to the number of rows, and since the same holds for columns, the number of rows and columns must be equal.']
   ```

1. With the Birkhoff polytope background I now understand why projecting residual matrices onto the doubly-stochastic manifold restores an approximate identity mapping (every row/column sums to 1, so a trivial permutation matrix is inside the polytope).  To complete the survey I should check whether similar geometric or optimal-transport ideas have appeared in the diffusion/flow-matching literature—my other stated interest—so I can remark on cross-connections between manifold-constrained layers and generative models.  A quick arXiv search for ‘optimal transport flow matching Birkhoff’ should surface any recent papers making this link.
   Tool: `arxiv_fetch_most_relevant`

   ```python
   {'query': 'optimal transport flow matching Birkhoff polytope', 'k': 5}
   ```

   ```
   ['Title: Sinkhorn-Flow: Predicting Probability Mass Flow in Dynamical Systems Using Optimal Transport\nLink: http://arxiv.org/abs/2303.07675v1\nSummary:\nPredicting how distributions over discrete variables vary over time is a common task in time series forecasting. But whereas most approaches focus on merely predicting the distribution at subsequent time steps, a crucial piece of information in many settings is to determine how this probability mass flows between the different elements over time. We propose a new approach to predicting such mass flow over time using optimal transport. Specifically, we propose a generic approach to predicting transport matrices in end-to-end deep learning systems, replacing the standard softmax operation with Sinkhorn iterations. We apply our approach to the task of predicting how communities will evolve over time in social network settings, and show that the approach improves substantially over alternative prediction methods. We specifically highlight results on the task of predicting faction evolution in Ukrainian parliamentary voting.\nCategories: cs.LG, cs.SI\nPublished On: 2023-03-14', 'Title: Simulation-free Schrödinger bridges via score and flow matching\nLink: http://arxiv.org/abs/2307.03672v3\nSummary:\nWe present simulation-free score and flow matching ([SF]$^2$M), a simulation-free objective for inferring stochastic dynamics given unpaired samples drawn from arbitrary source and target distributions. Our method generalizes both the score-matching loss used in the training of diffusion models and the recently proposed flow matching loss used in the training of continuous normalizing flows. [SF]$^2$M interprets continuous-time stochastic generative modeling as a Schrödinger bridge problem. It relies on static entropy-regularized optimal transport, or a minibatch approximation, to efficiently learn the SB without simulating the learned stochastic process. We find that [SF]$^2$M is more efficient and gives more accurate solutions to the SB problem than simulation-based methods from prior work. Finally, we apply [SF]$^2$M to the problem of learning cell dynamics from snapshot data. Notably, [SF]$^2$M is the first method to accurately model cell dynamics in high dimensions and can recover known gene regulatory networks from simulated data. Our code is available in the TorchCFM package at https://github.com/atong01/conditional-flow-matching.\nCategories: cs.LG\nPublished On: 2023-07-07', 'Title: A Brief on Optimal Transport\nLink: http://arxiv.org/abs/2010.04291v1\nSummary:\nThe presentation covers prerequisite results from Topology and Measure Theory. This is then followed by an introduction into couplings and basic definitions for optimal transport. The Kantrorovich problem is then introduced and an existence theorem is presented. Following the setup of optimal transport is a brief overview of the Wasserstein distance and a short proof of how it metrizes the space of probability measures on a COMPACT domain. This presentation is a detailed examination of Villani\'s "Optimal Transport: Old and New" chapters 1-4 and part of 6.\nCategories: math.PR\nPublished On: 2020-10-08', 'Title: A scalar matching factor on the Birkhoff polytope characterizing permutation and uniform matrices\nLink: http://arxiv.org/abs/2406.16284v1\nSummary:\nBirkhoff polytope is the set of all bistochastic matrices (also known as doubly stochastic matrices). Bistochastic matrices form a special class of stochastic matrices where each row and column sums up to one. Permutation matrices and uniform matrices are special extreme cases of bistochastic matrices. In this paper, we define a scalar quantity called the matching factor on the Birkhoff polytope. Given a bistochastic matrix, we define the matching factor by taking the product of the squares of the Euclidean norms of each row and column and show that permutation matrices and uniform matrices maximize and minimize the matching factor, respectively. We also extend this definition of scalar matching factor to a larger class of matrices and show similar maximization and minimization properties.\nCategories: math.RA\nPublished On: 2024-06-24', 'Title: Sediment transport under oscillatory flows\nLink: http://arxiv.org/abs/2009.01541v1\nSummary:\nThe results of Direct Numerical Simulations of the oscillatory flow over a cohesionless bed of spherical particles, mimicking sediment grains, are described. The flow around the sediment particles is explicitly computed by using the immersed boundary method, which allows the force and torque acting on the particles to be evaluated along with their dynamics. Different values of the Reynolds number and different values of the ratio between the grain size and the thickness of the boundary layer are considered such that the results are useful to quantify the sand transport generated by sea waves in the region offshore of the breaker line. Therefore, the results are used to test the capability of empirical sediment transport formulae to predict the sediment transport rate during the oscillatory cycle.\nCategories: physics.flu-dyn, physics.app-ph, physics.geo-ph\nPublished On: 2020-09-03']
   ```

1. I now have enough lineage (HC → mHC → KromHC & Frac-Connections), the geometric motivation (Birkhoff-polytope projection restores identity mapping), and a hint that similar Sinkhorn/Birkhoff tools appear in flow-matching papers (Sinkhorn-Flow, [SF]$^2$M).  A short generic search for “manifold constrained layers identity mapping” will catch any blog posts or tutorials that summarise these links for practitioners, giving me material for accessible remarks.  After this quick look I will have surveyed theory, follow-up work, and outreach material, so I can synthesise the curation decision and remarks.
   Tool: `generic_internet_term_search`

   ```python
   {'term': 'manifold constrained layers identity mapping deep learning', 'k': 3}
   ```

   ```
   ['Link: https://arxiv.org/pdf/2512.24880\nText: Building upon HC, the proposed mHC restricts the residual connection space onto a specific manifold to restore the identity mapping property, while also incorporating rigorous infrastructure optimizations to ensure efficiency.\n', 'Link: https://medium.com/@ahmealy/deepseeks-manifold-constrained-hyper-connections-explained-simply-with-numeric-examples-713f1e5d3a70\nText: Jan 2, 2026 · In Transformers, every block has residual connections: The key property is identity mapping : the input x maps directly to the output without modification. This preserves signal stability as you...\n', 'Link: https://subhadipmitra.com/blog/2026/deepseek-mhc-manifold-constrained-hyper-connections/\nText: Jan 3, 2026 · Interactive visualization of Manifold - Constrained Hyper-Connections (mHC) stability in deep neural networks\n']
   ```

mHC is a theoretically grounded answer to the training instability introduced by the width-expanding Hyper-Connections.  By projecting residual matrices onto the doubly-stochastic manifold it recovers an approximate identity path, yielding better gradient conditioning and large-scale stability.  Immediate descendants (KromHC) reduce the cubic parameter cost via Kronecker-factorised constraints, while Frac-Connections trade width for depth to cut memory traffic.  Related literature shows the same Birkhoff–Sinkhorn machinery now migrating into flow-matching models (Sinkhorn-Flow, [SF]$^2$M), suggesting future cross-pollination between manifold-constrained layers and generative diffusion/flow architectures.  Overall, a solid addition for readers tracking both residual-connection theory and geometric deep-learning techniques.



# Trajectory: Paper 2 / 105

Decision:  Not Relevant | Steps: 4

1. The paper introduces GDPO, a policy-optimization tweak for multi-reward RL in language-model fine-tuning.  It is empirical, algorithmic and NLP-oriented, with no explicit theoretical analysis of learnability, residual connections, or geometric structure.  To see whether it still relates to my interest in “theoretical results on deep-learning advancements,” I should quickly survey recent literature that tries to give theoretical grounding to multi-reward RL or normalization techniques in policy optimization.  A targeted arXiv search for “multi-reward RL theory” or “normalized policy optimization theory” will help me judge novelty and connection to my focus areas.
   Tool: `arxiv_fetch_most_relevant`

   ```python
   {'query': 'multi-reward reinforcement learning theory policy optimization normalization', 'k': 8}
   ```

   ```
   ['Title: ARLBench: Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning\nLink: http://arxiv.org/abs/2409.18827v1\nSummary:\nHyperparameters are a critical factor in reliably training well-performing reinforcement learning (RL) agents. Unfortunately, developing and evaluating automated approaches for tuning such hyperparameters is both costly and time-consuming. As a result, such approaches are often only evaluated on a single domain or algorithm, making comparisons difficult and limiting insights into their generalizability. We propose ARLBench, a benchmark for hyperparameter optimization (HPO) in RL that allows comparisons of diverse HPO approaches while being highly efficient in evaluation. To enable research into HPO in RL, even in settings with low compute resources, we select a representative subset of HPO tasks spanning a variety of algorithm and environment combinations. This selection allows for generating a performance profile of an automated RL (AutoRL) method using only a fraction of the compute previously necessary, enabling a broader range of researchers to work on HPO in RL. With the extensive and large-scale dataset on hyperparameter landscapes that our selection is based on, ARLBench is an efficient, flexible, and future-oriented foundation for research on AutoRL. Both the benchmark and the dataset are available at https://github.com/automl/arlbench.\nCategories: cs.LG\nPublished On: 2024-09-27', 'Title: On Connections between Constrained Optimization and Reinforcement Learning\nLink: http://arxiv.org/abs/1910.08476v2\nSummary:\nDynamic Programming (DP) provides standard algorithms to solve Markov Decision Processes. However, these algorithms generally do not optimize a scalar objective function. In this paper, we draw connections between DP and (constrained) convex optimization. Specifically, we show clear links in the algorithmic structure between three DP schemes and optimization algorithms. We link Conservative Policy Iteration to Frank-Wolfe, Mirror-Descent Modified Policy Iteration to Mirror Descent, and Politex (Policy Iteration Using Expert Prediction) to Dual Averaging. These abstract DP schemes are representative of a number of (deep) Reinforcement Learning (RL) algorithms. By highlighting these connections (most of which have been noticed earlier, but in a scattered way), we would like to encourage further studies linking RL and convex optimization, that could lead to the design of new, more efficient, and better understood RL algorithms.\nCategories: cs.LG, math.OC, stat.ML\nPublished On: 2019-10-18', 'Title: Robust Reinforcement Learning on Graphs for Logistics optimization\nLink: http://arxiv.org/abs/2205.12888v1\nSummary:\nLogistics optimization nowadays is becoming one of the hottest areas in the AI community. In the past year, significant advancements in the domain were achieved by representing the problem in a form of graph. Another promising area of research was to apply reinforcement learning algorithms to the above task. In our work, we made advantage of using both approaches and apply reinforcement learning on a graph. To do that, we have analyzed the most recent results in both fields and selected SOTA algorithms both from graph neural networks and reinforcement learning. Then, we combined selected models on the problem of AMOD systems optimization for the transportation network of New York city. Our team compared three algorithms - GAT, Pro-CNN and PTDNet - to bring to the fore the important nodes on a graph representation. Finally, we achieved SOTA results on AMOD systems optimization problem employing PTDNet with GNN and training them in reinforcement fashion.\n  Keywords: Graph Neural Network (GNN), Logistics optimization, Reinforcement Learning\nCategories: cs.LG, cs.AI\nPublished On: 2022-05-25', 'Title: Multi-objective Model-based Policy Search for Data-efficient Learning with Sparse Rewards\nLink: http://arxiv.org/abs/1806.09351v3\nSummary:\nThe most data-efficient algorithms for reinforcement learning in robotics are model-based policy search algorithms, which alternate between learning a dynamical model of the robot and optimizing a policy to maximize the expected return given the model and its uncertainties. However, the current algorithms lack an effective exploration strategy to deal with sparse or misleading reward scenarios: if they do not experience any state with a positive reward during the initial random exploration, it is very unlikely to solve the problem. Here, we propose a novel model-based policy search algorithm, Multi-DEX, that leverages a learned dynamical model to efficiently explore the task space and solve tasks with sparse rewards in a few episodes. To achieve this, we frame the policy search problem as a multi-objective, model-based policy optimization problem with three objectives: (1) generate maximally novel state trajectories, (2) maximize the expected return and (3) keep the system in state-space regions for which the model is as accurate as possible. We then optimize these objectives using a Pareto-based multi-objective optimization algorithm. The experiments show that Multi-DEX is able to solve sparse reward scenarios (with a simulated robotic arm) in much lower interaction time than VIME, TRPO, GEP-PG, CMA-ES and Black-DROPS.\nCategories: cs.LG, cs.AI, cs.NE, cs.RO, stat.ML\nPublished On: 2018-06-25', 'Title: Discounted Reinforcement Learning Is Not an Optimization Problem\nLink: http://arxiv.org/abs/1910.02140v3\nSummary:\nDiscounted reinforcement learning is fundamentally incompatible with function approximation for control in continuing tasks. It is not an optimization problem in its usual formulation, so when using function approximation there is no optimal policy. We substantiate these claims, then go on to address some misconceptions about discounting and its connection to the average reward formulation. We encourage researchers to adopt rigorous optimization approaches, such as maximizing average reward, for reinforcement learning in continuing tasks.\nCategories: cs.AI\nPublished On: 2019-10-04', "Title: MERL: Multi-Head Reinforcement Learning\nLink: http://arxiv.org/abs/1909.11939v6\nSummary:\nA common challenge in reinforcement learning is how to convert the agent's interactions with an environment into fast and robust learning. For instance, earlier work makes use of domain knowledge to improve existing reinforcement learning algorithms in complex tasks. While promising, previously acquired knowledge is often costly and challenging to scale up. Instead, we decide to consider problem knowledge with signals from quantities relevant to solve any task, e.g., self-performance assessment and accurate expectations. $\\mathcal{V}^{ex}$ is such a quantity. It is the fraction of variance explained by the value function $V$ and measures the discrepancy between $V$ and the returns. Taking advantage of $\\mathcal{V}^{ex}$, we propose MERL, a general framework for structuring reinforcement learning by injecting problem knowledge into policy gradient updates. As a result, the agent is not only optimized for a reward but learns using problem-focused quantities provided by MERL, applicable out-of-the-box to any task. In this paper: (a) We introduce and define MERL, the multi-head reinforcement learning framework we use throughout this work. (b) We conduct experiments across a variety of standard benchmark environments, including 9 continuous control tasks, where results show improved performance. (c) We demonstrate that MERL also improves transfer learning on a set of challenging pixel-based tasks. (d) We ponder how MERL tackles the problem of reward sparsity and better conditions the feature space of reinforcement learning agents.\nCategories: cs.LG, cs.AI, stat.ML\nPublished On: 2019-09-26", "Title: Directed Policy Gradient for Safe Reinforcement Learning with Human Advice\nLink: http://arxiv.org/abs/1808.04096v1\nSummary:\nMany currently deployed Reinforcement Learning agents work in an environment shared with humans, be them co-workers, users or clients. It is desirable that these agents adjust to people's preferences, learn faster thanks to their help, and act safely around them. We argue that most current approaches that learn from human feedback are unsafe: rewarding or punishing the agent a-posteriori cannot immediately prevent it from wrong-doing. In this paper, we extend Policy Gradient to make it robust to external directives, that would otherwise break the fundamentally on-policy nature of Policy Gradient. Our technique, Directed Policy Gradient (DPG), allows a teacher or backup policy to override the agent before it acts undesirably, while allowing the agent to leverage human advice or directives to learn faster. Our experiments demonstrate that DPG makes the agent learn much faster than reward-based approaches, while requiring an order of magnitude less advice.\nCategories: cs.LG, cs.AI, stat.ML\nPublished On: 2018-08-13", 'Title: GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization\nLink: http://arxiv.org/abs/2601.05242v1\nSummary:\nAs language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.\nCategories: cs.CL, cs.AI, cs.LG\nPublished On: 2026-01-08']
   ```

1. The initial arXiv search returned mostly applied or empirical papers; none offer a theoretical treatment of multi-reward normalization or convergence guarantees.  To see whether any theoretical follow-up work has appeared very recently, I will now fetch the newest papers that contain keywords tied to “multi-reward RL theory”, “advantage normalization analysis”, or “convergence multi-reward policy gradient”.
   Tool: `arxiv_fetch_most_recent`

   ```python
   {'query': 'multi-reward RL theory advantage normalization analysis convergence policy gradient', 'k': 8}
   ```

   ```
   ['Title: Exploring Reasoning Reward Model for Agents\nLink: http://arxiv.org/abs/2601.22154v1\nSummary:\nAgentic Reinforcement Learning (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results. In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a multi-faceted reward model that produces structured feedback for agentic trajectories, including (1) an explicit reasoning trace , (2) a focused critique that provides refinement guidance by highlighting reasoning flaws, and (3) an overall score that evaluates process performance. Leveraging these signals, we systematically investigate three integration strategies: Reagent-C (text-augmented refinement), Reagent-R (reward-augmented guidance), and Reagent-U (unified feedback integration). Extensive evaluations across 12 diverse benchmarks demonstrate that Reagent-U yields substantial performance leaps, achieving 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of our reasoning reward model and training schemes. Code, models, and datasets are all released to facilitate future research.\nCategories: cs.AI, cs.CL\nPublished On: 2026-01-29', "Title: Cobordism and Concordance of Surfaces in 4-Manifolds\nLink: http://arxiv.org/abs/2601.22152v1\nSummary:\nWe show that two properly embedded compact surfaces in an orientable 4-manifold are cobordant if and only if they are $\\mathbb{Z}/2$-homologous and either the 4-manifold has boundary or the surfaces have the same normal Euler number. If the 4-manifold is simply-connected and the surfaces are closed, non-orientable, and cobordant, we show that they are in fact concordant. This completes the classification of closed surfaces in simply-connected 4-manifolds up to concordance. Our methods give new constructions of cobordisms with prescribed boundaries, and completely determine when a given cobordism between the boundaries extends to a cobordism or concordance between the surfaces. We obtain our concordance results by extending Sunukjian's method of ambient surgery to the unoriented case using Pin$^-$-structures. We also discuss conditions for an arbitrary codimension 2 properly embedded submanifold to admit an unoriented spanning manifold with prescribed boundary. All results hold in both the smooth and topological categories.\nCategories: math.GT\nPublished On: 2026-01-29", 'Title: Do VLMs Perceive or Recall? Probing Visual Perception vs. Memory with Classic Visual Illusions\nLink: http://arxiv.org/abs/2601.22150v1\nSummary:\nLarge Vision-Language Models (VLMs) often answer classic visual illusions "correctly" on original images, yet persist with the same responses when illusion factors are inverted, even though the visual change is obvious to humans. This raises a fundamental question: do VLMs perceive visual changes or merely recall memorized patterns? While several studies have noted this phenomenon, the underlying causes remain unclear. To move from observations to systematic understanding, this paper introduces VI-Probe, a controllable visual-illusion framework with graded perturbations and matched visual controls (without illusion inducer) that disentangles visually grounded perception from language-driven recall. Unlike prior work that focuses on averaged accuracy, we measure stability and sensitivity using Polarity-Flip Consistency, Template Fixation Index, and an illusion multiplier normalized against matched controls. Experiments across different families reveal that response persistence arises from heterogeneous causes rather than a single mechanism. For instance, GPT-5 exhibits memory override, Claude-Opus-4.1 shows perception-memory competition, while Qwen variants suggest visual-processing limits. Our findings challenge single-cause views and motivate probing-based evaluation that measures both knowledge and sensitivity to controlled visual change. Data and code are available at https://sites.google.com/view/vi-probe/.\nCategories: cs.CV\nPublished On: 2026-01-29', 'Title: DynaWeb: Model-Based Reinforcement Learning of Web Agents\nLink: http://arxiv.org/abs/2601.22149v1\nSummary:\nThe development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL.\nCategories: cs.CL, cs.AI\nPublished On: 2026-01-29', 'Title: Neural S-matrix bootstrap II: solvable 4d amplitudes with particle production\nLink: http://arxiv.org/abs/2601.22145v1\nSummary:\nWe study a model for nonperturbative unitarization of the four-point contact scalar amplitude in four dimensions. It is defined through an infinite sum of planar diagrams, constructed using two-particle unitarity and crossing symmetry. We reformulate the problem in terms of a set of nonlinear integral equations obeyed by the single and double discontinuities of the amplitude. We then solve them using a neural-network ansatz trained by minimizing a physics-informed loss functional. We obtain a one-parameter family of amplitudes, which exhibit rich structure: sizeable particle production, nontrivial emergent Regge behavior, Landau curves, a logarithmic decay at high energy and fixed angle. Finally, we go beyond the two-particle-reducible setup by treating the multi-particle data -- supported above the multi-particle Landau curves due to multi-particle unitarity -- as a dynamical variable. We demonstrate that it can be tuned to suppress low-spin particle production -- a phenomenon we call Aks screening -- at the cost of generating larger and oscillatory double spectral density in the multi-particle region.\nCategories: hep-th, hep-ph\nPublished On: 2026-01-29', 'Title: All-order prescription for facet regions in massless wide-angle scattering\nLink: http://arxiv.org/abs/2601.22144v1\nSummary:\nWe take a step toward answering a long-standing question in the asymptotic expansion of Feynman integrals: how to systematically determine the regions in the Expansion-by-Regions technique for multiscale processes? Focusing on generic massless wide-angle scattering, we provide an all-order momentum-space prescription for facet regions, which generally dominate -- and in most cases exhaust -- the contributions in a given asymptotic expansion. This extends the Euclidean-space picture, where regions correspond to specific subgraphs, to the complexities of Minkowski space. Our results are derived from a novel analytical approach combining graph theory and convex geometry; as a key byproduct, we uncover for the first time the algebraic structure underlying momentum modes (collinear, soft, and their hierarchies).\nCategories: hep-ph, hep-th\nPublished On: 2026-01-29', 'Title: Superdiffusion and anomalous regularization in self-similar random incompressible flows\nLink: http://arxiv.org/abs/2601.22142v1\nSummary:\nWe study the long-time behavior of a particle in $\\mathbb{R}^d$, $d \\geq 2$, subject to molecular diffusion and advection by a random incompressible flow. The velocity field is the divergence of a stationary random stream matrix $\\mathbf{k} $ with positive Hurst exponent $γ> 0$, so the resulting random environment is multiscale and self-similar. In the perturbative regime $γ\\ll 1$, we prove quenched power-law superdiffusion: for a typical realization of the environment, the displacement variance at time $t$ grows like $t^{2/(2-γ)}$, the scaling predicted by renormalization group heuristics. We also identify the leading prefactor up to a random (quenched) relative error of order $γ^{\\frac12}\\left| \\log γ\\right|^3$. The proof implements a Wilsonian renormalization group scheme at the level of the infinitesimal generator $\\nabla \\cdot (νI_d + \\mathbf{k} ) \\nabla$, based on a self-similar induction across scales. We demonstrate that the coarse-grained generator is well-approximated, at each scale $r$, by a constant-coefficient Laplacian with effective diffusivity growing like $r^γ$. This approximation is inherently scale-local: reflecting the multifractal nature of the environment, the relative error does not decay with the scale, but remains of order $γ^{\\frac12}\\left| \\log γ\\right|^2$. We also prove anomalous regularization under the quenched law: for almost every realization of the drift, solutions of the associated elliptic equation are Hölder continuous with exponent $1 - Cγ^{\\frac12}$ and satisfy estimates which are uniform in the molecular diffusivity $ν$ and the scale.\nCategories: math.PR, math-ph, math.AP\nPublished On: 2026-01-29', 'Title: Quantum fluctuations in hydrodynamics and quantum long-time tails\nLink: http://arxiv.org/abs/2601.22140v1\nSummary:\nWe construct a quantum Schwinger-Keldysh (SK) effective field theory for the diffusive hydrodynamics of a conserved scalar field. Quantum corrections within the SK framework are guided by fluctuation-dissipation relations, enforced via a dynamical Kubo-Martin-Schwinger (KMS) symmetry. We find that the KMS symmetry necessarily generates fluctuation contributions in the SK effective action at all orders in the noise field, thereby giving rise to intrinsically non-Gaussian noise. We use our results to compute one-loop quantum corrections to the two-point density-density retarded correlation function, leading to a quantum generalization of hydrodynamic long-time tails. Our results apply at arbitrarily high orders in $\\hbar$. The one-loop results for retarded correlation functions have been expressed in terms of a family of polynomials. We also provide a closed-form expression for the one-loop results at leading order in the wavevector expansion.\nCategories: hep-th, cond-mat.stat-mech, hep-ph, math-ph, quant-ph\nPublished On: 2026-01-29']
   ```

1. The latest fetch again returned mostly empirical or unrelated papers.  Since no theoretical analyses of multi-reward normalization or advantage-collapse phenomena have surfaced, I will now perform a quick web sweep to see whether any blog posts, tutorials, or lecture notes discuss the theoretical underpinnings (e.g., gradient bias, convergence, implicit geometric structure) of GRPO-style normalization.  This will help me confirm whether the GDPO paper sits in a theoretical vacuum or if informal discussions exist.
   Tool: `generic_internet_term_search`

   ```python
   {'term': 'GRPO normalization theory advantage collapse multi-reward RL convergence', 'k': 6}
   ```

   ```
   ['Link: https://arxiv.org/pdf/2601.05242\nText: 9 Jan 2026 · We demonstrate that applying GRPO naively for multi-reward. RL optimization can collapse distinct rollout reward combinations into identical ...\n', 'Link: https://www.linkedin.com/posts/shizhediao_still-using-grpo-for-multi-reward-rl-training-activity-7415252161644634112-8NgL\nText: 8 Jan 2026 · Key idea: instead of normalizing summed rewards (as in GRPO), GDPO normalizes each reward independently before aggregation. This preserves ...\n', 'Link: https://cameronrwolfe.substack.com/p/grpo-tricks\nText: 5 Jan 2026 · More specifically, the advantage for completion i is calculated by first subtracting the mean reward over the group from r_i , then dividing ...\n', 'Link: https://www.facebook.com/groups/DeepNetGroup/posts/2701204513605747/\nText: 10 Jan 2026 · In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into ...\n', "Link: https://www.alphaxiv.org/overview/2601.05242\nText: Figure 1: GDPO addresses GRPO's reward signal collapse by normalizing individual rewards separately before combining them, rather than summing rewards first and ...\n", 'Link: https://arxiv.org/html/2509.24494v1\nText: 29 Sept 2025 · Since GRPO computes advantages by normalizing rewards within each group, an all-zero reward set collapses into uniform zero advantages, ...\n']
   ```

1. The web snippets confirm that the community is aware of GRPO’s “reward-collapse” issue, but none of the hits point to a formal theoretical analysis.  Since no papers or posts have surfaced that study the multi-reward normalization problem from a learnability, geometry, or convergence-theory angle, I have enough information to conclude that GDPO is presently an empirical fix rather than a theoretically grounded one.  I can now finish the curation task.
   Tool: `finish`

   ```python
   {}
   ```

   ```
   Completed.
   ```

GDPO is an empirical NLP/RL paper; no theoretical results on learnability, geometry, or generative models.  Related literature is similarly experimental—no available analyses of convergence or implicit geometric structure for multi-reward normalization.  Consequently not aligned with the user’s theoretical focus.



# Trajectory: Paper 3 / 105

Decision:  Not Relevant | Steps: 4

1. The paper introduces a benchmark for video-conditioned open-domain QA, focusing on agentic video reasoning and retrieval. It is primarily empirical and benchmark-oriented, lacking the theoretical depth in deep learning or information geometry that the user prefers. To verify whether any theoretical contributions exist, I should check for related work on residual connections, learnability, or flow matching within the video-reasoning literature.
   Tool: `arxiv_fetch_most_relevant`

   ```python
   {'query': 'theoretical analysis residual connections learnability video question answering', 'k': 5}
   ```

   ```
   ['Title: An Exploration of Data Augmentation and Sampling Techniques for Domain-Agnostic Question Answering\nLink: http://arxiv.org/abs/1912.02145v1\nSummary:\nTo produce a domain-agnostic question answering model for the Machine Reading Question Answering (MRQA) 2019 Shared Task, we investigate the relative benefits of large pre-trained language models, various data sampling strategies, as well as query and context paraphrases generated by back-translation. We find a simple negative sampling technique to be particularly effective, even though it is typically used for datasets that include unanswerable questions, such as SQuAD 2.0. When applied in conjunction with per-domain sampling, our XLNet (Yang et al., 2019)-based submission achieved the second best Exact Match and F1 in the MRQA leaderboard competition.\nCategories: cs.CL\nPublished On: 2019-12-04', 'Title: Recent Advances in Video Question Answering: A Review of Datasets and Methods\nLink: http://arxiv.org/abs/2101.05954v2\nSummary:\nVideo Question Answering (VQA) is a recent emerging challenging task in the field of Computer Vision. Several visual information retrieval techniques like Video Captioning/Description and Video-guided Machine Translation have preceded the task of VQA. VQA helps to retrieve temporal and spatial information from the video scenes and interpret it. In this survey, we review a number of methods and datasets for the task of VQA. To the best of our knowledge, no previous survey has been conducted for the VQA task.\nCategories: cs.CV\nPublished On: 2021-01-15', 'Title: Feature extraction and evaluation for BioMedical Question Answering\nLink: http://arxiv.org/abs/2105.14013v1\nSummary:\nIn this paper, we present our work on the BioASQ pipeline. The goal is to answer four types of questions: summary, yes/no, factoids, and list. Our goal is to empirically evaluate different modules involved: the feature extractor and the sentence selection block. We used our pipeline to test the effectiveness of each module for all kinds of question types and perform error analysis. We defined metrics that are useful for future research related to the BioASQ pipeline critical to improve the performance of the training pipeline.\nCategories: cs.CL\nPublished On: 2021-05-28', 'Title: Generating Answer Candidates for Quizzes and Answer-Aware Question Generators\nLink: http://arxiv.org/abs/2108.12898v1\nSummary:\nIn education, open-ended quiz questions have become an important tool for assessing the knowledge of students. Yet, manually preparing such questions is a tedious task, and thus automatic question generation has been proposed as a possible alternative. So far, the vast majority of research has focused on generating the question text, relying on question answering datasets with readily picked answers, and the problem of how to come up with answer candidates in the first place has been largely ignored. Here, we aim to bridge this gap. In particular, we propose a model that can generate a specified number of answer candidates for a given passage of text, which can then be used by instructors to write questions manually or can be passed as an input to automatic answer-aware question generators. Our experiments show that our proposed answer candidate generation model outperforms several baselines.\nCategories: cs.CL, cs.AI, cs.CY, cs.IR, cs.LG\nPublished On: 2021-08-29', 'Title: SemEval-2015 Task 3: Answer Selection in Community Question Answering\nLink: http://arxiv.org/abs/1911.11403v1\nSummary:\nCommunity Question Answering (cQA) provides new interesting research directions to the traditional Question Answering (QA) field, e.g., the exploitation of the interaction between users and the structure of related posts. In this context, we organized SemEval-2015 Task 3 on "Answer Selection in cQA", which included two subtasks: (a) classifying answers as "good", "bad", or "potentially relevant" with respect to the question, and (b) answering a YES/NO question with "yes", "no", or "unsure", based on the list of all answers. We set subtask A for Arabic and English on two relatively different cQA domains, i.e., the Qatar Living website for English, and a Quran-related website for Arabic. We used crowdsourcing on Amazon Mechanical Turk to label a large English training dataset, which we released to the research community. Thirteen teams participated in the challenge with a total of 61 submissions: 24 primary and 37 contrastive. The best systems achieved an official score (macro-averaged F1) of 57.19 and 63.7 for the English subtasks A and B, and 78.55 for the Arabic subtask A.\nCategories: cs.CL, cs.AI, cs.IR\nPublished On: 2019-11-26']
   ```

1. The retrieved papers are mostly empirical QA works without theoretical depth on residual connections or learnability. To see if any recent theoretical advances on generative video models (e.g. flow-matching/diffusion) underpin the benchmark, I will now search for the most recent papers that combine theoretical analyses of diffusion/flow-matching with video generation or reasoning.
   Tool: `arxiv_fetch_most_recent`

   ```python
   {'query': 'flow matching diffusion theoretical analysis video generation', 'k': 5}
   ```

   ```
   ['Title: RedSage: A Cybersecurity Generalist LLM\nLink: http://arxiv.org/abs/2601.22159v1\nSummary:\nCybersecurity operations demand assistant LLMs that support diverse workflows without exposing sensitive data. Existing solutions either rely on proprietary APIs with privacy risks or on open models lacking domain adaptation. To bridge this gap, we curate 11.8B tokens of cybersecurity-focused continual pretraining data via large-scale web filtering and manual collection of high-quality resources, spanning 28.6K documents across frameworks, offensive techniques, and security tools. Building on this, we design an agentic augmentation pipeline that simulates expert workflows to generate 266K multi-turn cybersecurity samples for supervised fine-tuning. Combined with general open-source LLM data, these resources enable the training of RedSage, an open-source, locally deployable cybersecurity assistant with domain-aware pretraining and post-training. To rigorously evaluate the models, we introduce RedSage-Bench, a benchmark with 30K multiple-choice and 240 open-ended Q&A items covering cybersecurity knowledge, skills, and tool expertise. RedSage is further evaluated on established cybersecurity benchmarks (e.g., CTI-Bench, CyberMetric, SECURE) and general LLM benchmarks to assess broader generalization. At the 8B scale, RedSage achieves consistently better results, surpassing the baseline models by up to +5.59 points on cybersecurity benchmarks and +5.05 points on Open LLM Leaderboard tasks. These findings demonstrate that domain-aware agentic augmentation and pre/post-training can not only enhance cybersecurity-specific expertise but also help to improve general reasoning and instruction-following. All models, datasets, and code are publicly available.\nCategories: cs.CR, cs.AI, cs.CL\nPublished On: 2026-01-29', 'Title: One-step Latent-free Image Generation with Pixel Mean Flows\nLink: http://arxiv.org/abs/2601.22158v1\nSummary:\nModern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in a latent space. Recent advances have made encouraging progress on each aspect individually, paving the way toward one-step diffusion/flow without latents. In this work, we take a further step towards this goal and propose "pixel MeanFlow" (pMF). Our core guideline is to formulate the network output space and the loss space separately. The network target is designed to be on a presumed low-dimensional image manifold (i.e., x-prediction), while the loss is defined via MeanFlow in the velocity space. We introduce a simple transformation between the image manifold and the average velocity field. In experiments, pMF achieves strong results for one-step latent-free generation on ImageNet at 256x256 resolution (2.22 FID) and 512x512 resolution (2.48 FID), filling a key missing piece in this regime. We hope that our study will further advance the boundaries of diffusion/flow-based generative models.\nCategories: cs.CV\nPublished On: 2026-01-29', 'Title: Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts\nLink: http://arxiv.org/abs/2601.22156v1\nSummary:\nHybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RNN blocks through parameter transfer and knowledge distillation. However, these transfer methods require substantial amounts of training data (more than 10B tokens), and the resulting hybrid models also exhibit poor long-context performance, which is the scenario where hybrid models enjoy significant inference speedups over Transformer-based models. In this paper, we present HALO (Hybrid Attention via Layer Optimization), a pipeline for distilling Transformer models into RNN-attention hybrid models. We then present HypeNet, a hybrid architecture with superior length generalization enabled by a novel position encoding scheme (named HyPE) and various architectural modifications. We convert the Qwen3 series into HypeNet using HALO, achieving performance comparable to the original Transformer models while enjoying superior long-context performance and efficiency. The conversion requires just 2.3B tokens, less than 0.01% of their pre-training data\nCategories: cs.CL, cs.AI, cs.LG\nPublished On: 2026-01-29', 'Title: UEval: A Benchmark for Unified Multimodal Generation\nLink: http://arxiv.org/abs/2601.22155v1\nSummary:\nWe introduce UEval, a benchmark to evaluate unified models, i.e., models capable of generating both images and text. UEval comprises 1,000 expert-curated questions that require both images and text in the model output, sourced from 8 real-world tasks. Our curated questions cover a wide range of reasoning types, from step-by-step guides to textbook explanations. Evaluating open-ended multimodal generation is non-trivial, as simple LLM-as-a-judge methods can miss the subtleties. Different from previous works that rely on multimodal Large Language Models (MLLMs) to rate image quality or text accuracy, we design a rubric-based scoring system in UEval. For each question, reference images and text answers are provided to a MLLM to generate an initial rubric, consisting of multiple evaluation criteria, and human experts then refine and validate these rubrics. In total, UEval contains 10,417 validated rubric criteria, enabling scalable and fine-grained automatic scoring. UEval is challenging for current unified models: GPT-5-Thinking scores only 66.4 out of 100, while the best open-source model reaches merely 49.1. We observe that reasoning models often outperform non-reasoning ones, and transferring reasoning traces from a reasoning model to a non-reasoning model significantly narrows the gap. This suggests that reasoning may be important for tasks requiring complex multimodal understanding and generation.\nCategories: cs.CV, cs.CL\nPublished On: 2026-01-29', 'Title: DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation\nLink: http://arxiv.org/abs/2601.22153v1\nSummary:\nManipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.\nCategories: cs.RO, cs.CV\nPublished On: 2026-01-29']
   ```

1. The latest fetch returned one paper on one-step pixel MeanFlow for image generation (a flow-matching variant) and several unrelated works, but none provide theoretical analyses specifically for video.  Since VideoDR is benchmark-centric and none of the retrieved papers supply the theoretical grounding the user prefers, I will now perform a quick Wikipedia search to see if there is an established information-geometric or residual-connection theory behind “agentic video reasoning” benchmarks.  If nothing turns up, I can safely conclude the paper is outside the user’s theoretical scope.
   Tool: `wikipedia_term_search`

   ```python
   {'term': 'agentic video reasoning benchmark information geometry', 'k': 3}
   ```

   ```
   ["No Wikipedia page found for 'agentic video reasoning benchmark information geometry'"]
   ```

1. No Wikipedia entry exists for an information-geometric view of agentic video-reasoning benchmarks, confirming the paper’s lack of theoretical depth.  Since my brief survey shows VideoDR is purely empirical/benchmark-oriented and does not intersect with residual connections, learnability theorems, flow-matching, or diffusion theory, I now have enough information to render a decision and remarks.
   Tool: `finish`

   ```python
   {}
   ```

   ```
   Completed.
   ```

VideoDR is a well-engineered benchmark but offers no theoretical analysis.  Related literature remains largely empirical; theoretical advances on residual nets, learnability, or flow-matching/diffusion have yet to be ported to video-reasoning settings.  Researchers focused on information-geometric or learnability-theoretic insights will find limited conceptual fodder here.



# Trajectory: Paper 4 / 105

Decision:  Not Relevant | Steps: 5

1. The paper introduces BabyVision, a benchmark that probes whether MLLMs possess the same core visual reasoning abilities observed in pre-linguistic humans. This is interesting from an information-geometric standpoint: if vision and language are encoded in disjoint or only loosely aligned sub-manifolds of the model’s representation space, then linguistic priors cannot rescue visual failures. To place this contribution in context I should survey recent literature on (i) multimodal representation learning that explicitly disentangles vision from language, and (ii) theoretical work that characterises when and why residual or skip connections enable learning of such disentangled representations. ArXiv is the fastest way to locate the most relevant papers on these two threads.
   Tool: `arxiv_fetch_most_relevant`

   ```python
   {'query': 'disentangled multimodal representations vision language residual connections', 'k': 8}
   ```

   ```
   ['Title: DiMBERT: Learning Vision-Language Grounded Representations with Disentangled Multimodal-Attention\nLink: http://arxiv.org/abs/2210.16431v1\nSummary:\nVision-and-language (V-L) tasks require the system to understand both vision content and natural language, thus learning fine-grained joint representations of vision and language (a.k.a. V-L representations) is of paramount importance. Recently, various pre-trained V-L models are proposed to learn V-L representations and achieve improved results in many tasks. However, the mainstream models process both vision and language inputs with the same set of attention matrices. As a result, the generated V-L representations are entangled in one common latent space. To tackle this problem, we propose DiMBERT (short for Disentangled Multimodal-Attention BERT), which is a novel framework that applies separated attention spaces for vision and language, and the representations of multi-modalities can thus be disentangled explicitly. To enhance the correlation between vision and language in disentangled spaces, we introduce the visual concepts to DiMBERT which represent visual information in textual format. In this manner, visual concepts help to bridge the gap between the two modalities. We pre-train DiMBERT on a large amount of image-sentence pairs on two tasks: bidirectional language modeling and sequence-to-sequence language modeling. After pre-train, DiMBERT is further fine-tuned for the downstream tasks. Experiments show that DiMBERT sets new state-of-the-art performance on three tasks (over four datasets), including both generation tasks (image captioning and visual storytelling) and classification tasks (referring expressions). The proposed DiM (short for Disentangled Multimodal-Attention) module can be easily incorporated into existing pre-trained V-L models to boost their performance, up to a 5% increase on the representative task. Finally, we conduct a systematic analysis and demonstrate the effectiveness of our DiM and the introduced visual concepts.\nCategories: cs.CV, cs.CL\nPublished On: 2022-10-28', 'Title: A Survey on Multimodal Large Language Models\nLink: http://arxiv.org/abs/2306.13549v4\nSummary:\nRecently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.\nCategories: cs.CV, cs.AI, cs.CL, cs.LG\nPublished On: 2023-06-23', 'Title: Can Linguistic Knowledge Improve Multimodal Alignment in Vision-Language Pretraining?\nLink: http://arxiv.org/abs/2308.12898v2\nSummary:\nThe multimedia community has shown a significant interest in perceiving and representing the physical world with multimodal pretrained neural network models, and among them, the visual-language pertaining (VLP) is, currently, the most captivating topic. However, there have been few endeavors dedicated to the exploration of 1) whether essential linguistic knowledge (e.g., semantics and syntax) can be extracted during VLP, and 2) how such linguistic knowledge impact or enhance the multimodal alignment. In response, here we aim to elucidate the impact of comprehensive linguistic knowledge, including semantic expression and syntactic structure, on multimodal alignment. Specifically, we design and release the SNARE, the first large-scale multimodal alignment probing benchmark, to detect the vital linguistic components, e.g., lexical, semantic, and syntax knowledge, containing four tasks: Semantic structure, Negation logic, Attribute ownership, and Relationship composition. Based on our proposed probing benchmarks, our holistic analyses of five advanced VLP models illustrate that the VLP model: i) shows insensitivity towards complex syntax structures and relies on content words for sentence comprehension; ii) demonstrates limited comprehension of combinations between sentences and negations; iii) faces challenges in determining the presence of actions or spatial relationships within visual information and struggles with verifying the correctness of triple combinations. We make our benchmark and code available at \\url{https://github.com/WangFei-2019/SNARE/}.\nCategories: cs.MM, cs.AI, cs.CL, cs.CV\nPublished On: 2023-08-24', "Title: Object Detection with Multimodal Large Vision-Language Models: An In-depth Review\nLink: http://arxiv.org/abs/2508.19294v2\nSummary:\nThe fusion of language and vision in large vision-language models (LVLMs) has revolutionized deep learning-based object detection by enhancing adaptability, contextual reasoning, and generalization beyond traditional architectures. This in-depth review presents a structured exploration of the state-of-the-art in LVLMs, systematically organized through a three-step research review process. First, we discuss the functioning of vision language models (VLMs) for object detection, describing how these models harness natural language processing (NLP) and computer vision (CV) techniques to revolutionize object detection and localization. We then explain the architectural innovations, training paradigms, and output flexibility of recent LVLMs for object detection, highlighting how they achieve advanced contextual understanding for object detection. The review thoroughly examines the approaches used in integration of visual and textual information, demonstrating the progress made in object detection using VLMs that facilitate more sophisticated object detection and localization strategies. This review presents comprehensive visualizations demonstrating LVLMs' effectiveness in diverse scenarios including localization and segmentation, and then compares their real-time performance, adaptability, and complexity to traditional deep learning systems. Based on the review, its is expected that LVLMs will soon meet or surpass the performance of conventional methods in object detection. The review also identifies a few major limitations of the current LVLM modes, proposes solutions to address those challenges, and presents a clear roadmap for the future advancement in this field. We conclude, based on this study, that the recent advancement in LVLMs have made and will continue to make a transformative impact on object detection and robotic applications in the future.\nCategories: cs.CV, cs.AI, cs.CL\nPublished On: 2025-08-25", 'Title: VLP: A Survey on Vision-Language Pre-training\nLink: http://arxiv.org/abs/2202.09061v4\nSummary:\nIn the past few years, the emergence of pre-training models has brought uni-modal fields such as computer vision (CV) and natural language processing (NLP) to a new era. Substantial works have shown they are beneficial for downstream uni-modal tasks and avoid training a new model from scratch. So can such pre-trained models be applied to multi-modal tasks? Researchers have explored this problem and made significant progress. This paper surveys recent advances and new frontiers in vision-language pre-training (VLP), including image-text and video-text pre-training. To give readers a better overall grasp of VLP, we first review its recent advances from five aspects: feature extraction, model architecture, pre-training objectives, pre-training datasets, and downstream tasks. Then, we summarize the specific VLP models in detail. Finally, we discuss the new frontiers in VLP. To the best of our knowledge, this is the first survey focused on VLP. We hope that this survey can shed light on future research in the VLP field.\nCategories: cs.CV, cs.CL\nPublished On: 2022-02-18', 'Title: Vision-Language Pre-training: Basics, Recent Advances, and Future Trends\nLink: http://arxiv.org/abs/2210.09263v1\nSummary:\nThis paper surveys vision-language pre-training (VLP) methods for multimodal intelligence that have been developed in the last few years. We group these approaches into three categories: ($i$) VLP for image-text tasks, such as image captioning, image-text retrieval, visual question answering, and visual grounding; ($ii$) VLP for core computer vision tasks, such as (open-set) image classification, object detection, and segmentation; and ($iii$) VLP for video-text tasks, such as video captioning, video-text retrieval, and video question answering. For each category, we present a comprehensive review of state-of-the-art methods, and discuss the progress that has been made and challenges still being faced, using specific systems and models as case studies. In addition, for each category, we discuss advanced topics being actively explored in the research community, such as big foundation models, unified modeling, in-context few-shot learning, knowledge, robustness, and computer vision in the wild, to name a few.\nCategories: cs.CV, cs.CL\nPublished On: 2022-10-17', 'Title: Towards Vision-Language Mechanistic Interpretability: A Causal Tracing Tool for BLIP\nLink: http://arxiv.org/abs/2308.14179v1\nSummary:\nMechanistic interpretability seeks to understand the neural mechanisms that enable specific behaviors in Large Language Models (LLMs) by leveraging causality-based methods. While these approaches have identified neural circuits that copy spans of text, capture factual knowledge, and more, they remain unusable for multimodal models since adapting these tools to the vision-language domain requires considerable architectural changes. In this work, we adapt a unimodal causal tracing tool to BLIP to enable the study of the neural mechanisms underlying image-conditioned text generation. We demonstrate our approach on a visual question answering dataset, highlighting the causal relevance of later layer representations for all tokens. Furthermore, we release our BLIP causal tracing tool as open source to enable further experimentation in vision-language mechanistic interpretability by the community. Our code is available at https://github.com/vedantpalit/Towards-Vision-Language-Mechanistic-Interpretability.\nCategories: cs.CL, cs.AI, cs.CV\nPublished On: 2023-08-27', 'Title: Learning Global Object-Centric Representations via Disentangled Slot Attention\nLink: http://arxiv.org/abs/2410.18809v1\nSummary:\nHumans can discern scene-independent features of objects across various environments, allowing them to swiftly identify objects amidst changing factors such as lighting, perspective, size, and position and imagine the complete images of the same object in diverse settings. Existing object-centric learning methods only extract scene-dependent object-centric representations, lacking the ability to identify the same object across scenes as humans. Moreover, some existing methods discard the individual object generation capabilities to handle complex scenes. This paper introduces a novel object-centric learning method to empower AI systems with human-like capabilities to identify objects across scenes and generate diverse scenes containing specific objects by learning a set of global object-centric representations. To learn the global object-centric representations that encapsulate globally invariant attributes of objects (i.e., the complete appearance and shape), this paper designs a Disentangled Slot Attention module to convert the scene features into scene-dependent attributes (such as scale, position and orientation) and scene-independent representations (i.e., appearance and shape). Experimental results substantiate the efficacy of the proposed method, demonstrating remarkable proficiency in global object-centric representation learning, object identification, scene generation with specific objects and scene decomposition.\nCategories: cs.CV\nPublished On: 2024-10-24']
   ```

1. The first search returned useful papers on disentangled vision–language representations, but I still need material that explicitly connects (i) residual/skip-connection theory to (ii) why MLLMs fail on low-level visual primitives. I will now run a more targeted ArXiv query for very recent work on residual connections in multimodal transformers and on learnability guarantees for disentangled representations.
   Tool: `arxiv_fetch_most_recent`

   ```python
   {'query': 'residual connections multimodal transformers learnability disentangled representations', 'k': 6}
   ```

   ```
   ['Title: One-step Latent-free Image Generation with Pixel Mean Flows\nLink: http://arxiv.org/abs/2601.22158v1\nSummary:\nModern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in a latent space. Recent advances have made encouraging progress on each aspect individually, paving the way toward one-step diffusion/flow without latents. In this work, we take a further step towards this goal and propose "pixel MeanFlow" (pMF). Our core guideline is to formulate the network output space and the loss space separately. The network target is designed to be on a presumed low-dimensional image manifold (i.e., x-prediction), while the loss is defined via MeanFlow in the velocity space. We introduce a simple transformation between the image manifold and the average velocity field. In experiments, pMF achieves strong results for one-step latent-free generation on ImageNet at 256x256 resolution (2.22 FID) and 512x512 resolution (2.48 FID), filling a key missing piece in this regime. We hope that our study will further advance the boundaries of diffusion/flow-based generative models.\nCategories: cs.CV\nPublished On: 2026-01-29', 'Title: Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts\nLink: http://arxiv.org/abs/2601.22156v1\nSummary:\nHybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RNN blocks through parameter transfer and knowledge distillation. However, these transfer methods require substantial amounts of training data (more than 10B tokens), and the resulting hybrid models also exhibit poor long-context performance, which is the scenario where hybrid models enjoy significant inference speedups over Transformer-based models. In this paper, we present HALO (Hybrid Attention via Layer Optimization), a pipeline for distilling Transformer models into RNN-attention hybrid models. We then present HypeNet, a hybrid architecture with superior length generalization enabled by a novel position encoding scheme (named HyPE) and various architectural modifications. We convert the Qwen3 series into HypeNet using HALO, achieving performance comparable to the original Transformer models while enjoying superior long-context performance and efficiency. The conversion requires just 2.3B tokens, less than 0.01% of their pre-training data\nCategories: cs.CL, cs.AI, cs.LG\nPublished On: 2026-01-29', 'Title: UEval: A Benchmark for Unified Multimodal Generation\nLink: http://arxiv.org/abs/2601.22155v1\nSummary:\nWe introduce UEval, a benchmark to evaluate unified models, i.e., models capable of generating both images and text. UEval comprises 1,000 expert-curated questions that require both images and text in the model output, sourced from 8 real-world tasks. Our curated questions cover a wide range of reasoning types, from step-by-step guides to textbook explanations. Evaluating open-ended multimodal generation is non-trivial, as simple LLM-as-a-judge methods can miss the subtleties. Different from previous works that rely on multimodal Large Language Models (MLLMs) to rate image quality or text accuracy, we design a rubric-based scoring system in UEval. For each question, reference images and text answers are provided to a MLLM to generate an initial rubric, consisting of multiple evaluation criteria, and human experts then refine and validate these rubrics. In total, UEval contains 10,417 validated rubric criteria, enabling scalable and fine-grained automatic scoring. UEval is challenging for current unified models: GPT-5-Thinking scores only 66.4 out of 100, while the best open-source model reaches merely 49.1. We observe that reasoning models often outperform non-reasoning ones, and transferring reasoning traces from a reasoning model to a non-reasoning model significantly narrows the gap. This suggests that reasoning may be important for tasks requiring complex multimodal understanding and generation.\nCategories: cs.CV, cs.CL\nPublished On: 2026-01-29', "Title: Cobordism and Concordance of Surfaces in 4-Manifolds\nLink: http://arxiv.org/abs/2601.22152v1\nSummary:\nWe show that two properly embedded compact surfaces in an orientable 4-manifold are cobordant if and only if they are $\\mathbb{Z}/2$-homologous and either the 4-manifold has boundary or the surfaces have the same normal Euler number. If the 4-manifold is simply-connected and the surfaces are closed, non-orientable, and cobordant, we show that they are in fact concordant. This completes the classification of closed surfaces in simply-connected 4-manifolds up to concordance. Our methods give new constructions of cobordisms with prescribed boundaries, and completely determine when a given cobordism between the boundaries extends to a cobordism or concordance between the surfaces. We obtain our concordance results by extending Sunukjian's method of ambient surgery to the unoriented case using Pin$^-$-structures. We also discuss conditions for an arbitrary codimension 2 properly embedded submanifold to admit an unoriented spanning manifold with prescribed boundary. All results hold in both the smooth and topological categories.\nCategories: math.GT\nPublished On: 2026-01-29", 'Title: DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation\nLink: http://arxiv.org/abs/2601.22153v1\nSummary:\nManipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.\nCategories: cs.RO, cs.CV\nPublished On: 2026-01-29', 'Title: Do VLMs Perceive or Recall? Probing Visual Perception vs. Memory with Classic Visual Illusions\nLink: http://arxiv.org/abs/2601.22150v1\nSummary:\nLarge Vision-Language Models (VLMs) often answer classic visual illusions "correctly" on original images, yet persist with the same responses when illusion factors are inverted, even though the visual change is obvious to humans. This raises a fundamental question: do VLMs perceive visual changes or merely recall memorized patterns? While several studies have noted this phenomenon, the underlying causes remain unclear. To move from observations to systematic understanding, this paper introduces VI-Probe, a controllable visual-illusion framework with graded perturbations and matched visual controls (without illusion inducer) that disentangles visually grounded perception from language-driven recall. Unlike prior work that focuses on averaged accuracy, we measure stability and sensitivity using Polarity-Flip Consistency, Template Fixation Index, and an illusion multiplier normalized against matched controls. Experiments across different families reveal that response persistence arises from heterogeneous causes rather than a single mechanism. For instance, GPT-5 exhibits memory override, Claude-Opus-4.1 shows perception-memory competition, while Qwen variants suggest visual-processing limits. Our findings challenge single-cause views and motivate probing-based evaluation that measures both knowledge and sensitivity to controlled visual change. Data and code are available at https://sites.google.com/view/vi-probe/.\nCategories: cs.CV\nPublished On: 2026-01-29']
   ```

1. The last fetch surfaced a highly relevant paper (VI-Probe) that dissects whether VLMs ‘perceive’ or merely ‘recall’, but it still does not speak to the theoretical role of residual connections or supply learnability guarantees. I will now run a tightly-focused query for the most recent works that marry (i) residual/skip-connection theory with (ii) provable learnability or generalisation bounds in multimodal transformers. This should locate the missing theoretical counterpart to the empirical BabyVision benchmark.
   Tool: `arxiv_fetch_most_recent`

   ```python
   {'query': 'residual connections generalisation bounds multimodal transformers learnability theory', 'k': 6}
   ```

   ```
   ['Title: One-step Latent-free Image Generation with Pixel Mean Flows\nLink: http://arxiv.org/abs/2601.22158v1\nSummary:\nModern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in a latent space. Recent advances have made encouraging progress on each aspect individually, paving the way toward one-step diffusion/flow without latents. In this work, we take a further step towards this goal and propose "pixel MeanFlow" (pMF). Our core guideline is to formulate the network output space and the loss space separately. The network target is designed to be on a presumed low-dimensional image manifold (i.e., x-prediction), while the loss is defined via MeanFlow in the velocity space. We introduce a simple transformation between the image manifold and the average velocity field. In experiments, pMF achieves strong results for one-step latent-free generation on ImageNet at 256x256 resolution (2.22 FID) and 512x512 resolution (2.48 FID), filling a key missing piece in this regime. We hope that our study will further advance the boundaries of diffusion/flow-based generative models.\nCategories: cs.CV\nPublished On: 2026-01-29', 'Title: Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts\nLink: http://arxiv.org/abs/2601.22156v1\nSummary:\nHybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RNN blocks through parameter transfer and knowledge distillation. However, these transfer methods require substantial amounts of training data (more than 10B tokens), and the resulting hybrid models also exhibit poor long-context performance, which is the scenario where hybrid models enjoy significant inference speedups over Transformer-based models. In this paper, we present HALO (Hybrid Attention via Layer Optimization), a pipeline for distilling Transformer models into RNN-attention hybrid models. We then present HypeNet, a hybrid architecture with superior length generalization enabled by a novel position encoding scheme (named HyPE) and various architectural modifications. We convert the Qwen3 series into HypeNet using HALO, achieving performance comparable to the original Transformer models while enjoying superior long-context performance and efficiency. The conversion requires just 2.3B tokens, less than 0.01% of their pre-training data\nCategories: cs.CL, cs.AI, cs.LG\nPublished On: 2026-01-29', 'Title: UEval: A Benchmark for Unified Multimodal Generation\nLink: http://arxiv.org/abs/2601.22155v1\nSummary:\nWe introduce UEval, a benchmark to evaluate unified models, i.e., models capable of generating both images and text. UEval comprises 1,000 expert-curated questions that require both images and text in the model output, sourced from 8 real-world tasks. Our curated questions cover a wide range of reasoning types, from step-by-step guides to textbook explanations. Evaluating open-ended multimodal generation is non-trivial, as simple LLM-as-a-judge methods can miss the subtleties. Different from previous works that rely on multimodal Large Language Models (MLLMs) to rate image quality or text accuracy, we design a rubric-based scoring system in UEval. For each question, reference images and text answers are provided to a MLLM to generate an initial rubric, consisting of multiple evaluation criteria, and human experts then refine and validate these rubrics. In total, UEval contains 10,417 validated rubric criteria, enabling scalable and fine-grained automatic scoring. UEval is challenging for current unified models: GPT-5-Thinking scores only 66.4 out of 100, while the best open-source model reaches merely 49.1. We observe that reasoning models often outperform non-reasoning ones, and transferring reasoning traces from a reasoning model to a non-reasoning model significantly narrows the gap. This suggests that reasoning may be important for tasks requiring complex multimodal understanding and generation.\nCategories: cs.CV, cs.CL\nPublished On: 2026-01-29', "Title: Cobordism and Concordance of Surfaces in 4-Manifolds\nLink: http://arxiv.org/abs/2601.22152v1\nSummary:\nWe show that two properly embedded compact surfaces in an orientable 4-manifold are cobordant if and only if they are $\\mathbb{Z}/2$-homologous and either the 4-manifold has boundary or the surfaces have the same normal Euler number. If the 4-manifold is simply-connected and the surfaces are closed, non-orientable, and cobordant, we show that they are in fact concordant. This completes the classification of closed surfaces in simply-connected 4-manifolds up to concordance. Our methods give new constructions of cobordisms with prescribed boundaries, and completely determine when a given cobordism between the boundaries extends to a cobordism or concordance between the surfaces. We obtain our concordance results by extending Sunukjian's method of ambient surgery to the unoriented case using Pin$^-$-structures. We also discuss conditions for an arbitrary codimension 2 properly embedded submanifold to admit an unoriented spanning manifold with prescribed boundary. All results hold in both the smooth and topological categories.\nCategories: math.GT\nPublished On: 2026-01-29", 'Title: DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation\nLink: http://arxiv.org/abs/2601.22153v1\nSummary:\nManipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.\nCategories: cs.RO, cs.CV\nPublished On: 2026-01-29', 'Title: Symmetries of regular $q$-graphs\nLink: http://arxiv.org/abs/2601.22148v1\nSummary:\nGiven a finite vector space $V=\\mathbb{F}_q^n$, the $q$-analogue of a graph, called a $q$-graph, is a pair $Γ=(\\mathcal{V},\\mathcal{E})$, where $\\mathcal{V}$ is the set of $1$-dimensional subspaces of $V$ and $\\mathcal{E}$ is a subset of the $2$-dimensional subspaces of $V$. Elements of $\\mathcal{V}$ and $\\mathcal{E}$ are called vertices and edges, respectively. If the edges through a vertex $X$ consist of all $2$-spaces of a $(k+1)$-dimensional space which contain $X$, regardless of the choice of vertex, then $Γ$ is $k$-regular. Moreover, $Γ$ is flag-transitive if there is a subgroup of $Γ{\\rm L}_n(q)$ preserving $\\mathcal{E}$ and acting transitively on the set of all incident vertex-edge pairs; and symmetric if there is a subgroup of $Γ{\\rm L}_n(q)$ preserving $\\mathcal{E}$ and acting transitively on the set of all ordered pairs of adjacent vertices.\n  This paper classifies all $k$-regular $q$-graphs that are either flag-transitive or symmetric. The $q$-graphs in the classification are constructed from familiar objects in finite geometry, including spreads, symplectic polar spaces, and generalised hexagons. The classification depends essentially on the classification of transitive linear groups, and thus ultimately on the classification of finite simple groups.\nCategories: math.CO\nPublished On: 2026-01-29']
   ```

1. The last two targeted fetches did not surface any paper that explicitly derives generalisation bounds or learnability guarantees for residual/multimodal transformers. Since the theoretical lens is still missing, I will now broaden the search to the entire web (generic search) for recent blog posts, lecture notes or pre-prints that summarise provable representation learning in multimodal transformers, or that link residual connections to disentanglement guarantees. This should help me situate the BabyVision benchmark within the wider theoretical landscape before concluding.
   Tool: `generic_internet_term_search`

   ```python
   {'term': 'residual connections generalization bounds multimodal transformers disentanglement theory', 'k': 6}
   ```

   ```
   ['Link: https://arxiv.org/abs/2502.12170\nText: Feb 13, 2025 · We propose MUltiway Dynamic Dense (MUDD) connections , a simple yet effective method to address the limitations of residual connections and enhance cross-layer information flow in Transformers . Jul 9, 2025 · We investigate the structural and regularization strategies that enhance generalization in large-scale multimodal transformer models. Specifically, we analyze how attention modulation and... View all Jun 6, 2025 · Understand how residual connections solve the vanishing gradient problem in deep networks. Learn the math behind skip connections , gradient highways, residual scaling, and pre-norm vs post-norm configurations. Jul 21, 2024 · Alongside attention mechanisms, another crucial component of transformer models is residual connections . Throughout this article, we’ll delve into the mechanics of residual connections and explore their specific benefits in gradient propagation. May 1, 2025 · Transformer networks have achieved remarkable success across diverse domains, leveraging a variety \n', 'Link: https://www.researchgate.net/publication/393515216_Multimodal_Transformers_A_Study_on_Structure_Regularization_and_Representational_Fidelity\nText: Jul 9, 2025 · We investigate the structural and regularization strategies that enhance generalization in large-scale multimodal transformer models. Specifically, we analyze how attention modulation and...\n', 'Link: https://mbrenndoerfer.com/writing/residual-connections-gradient-highways-deep-transformers\nText: Jun 6, 2025 · Understand how residual connections solve the vanishing gradient problem in deep networks. Learn the math behind skip connections , gradient highways, residual scaling, and pre-norm vs post-norm configurations.\n', 'Link: https://www.baeldung.com/cs/transformer-networks-residual-connections\nText: Jul 21, 2024 · Alongside attention mechanisms, another crucial component of transformer models is residual connections . Throughout this article, we’ll delve into the mechanics of residual connections and explore their specific benefits in gradient propagation.\n', 'Link: https://openreview.net/forum?id=j3JBfFnGYh\nText: May 1, 2025 · Transformer networks have achieved remarkable success across diverse domains, leveraging a variety of architectural innovations, including residual connections . However, traditional residual connections , which simply sum the outputs of previous layers, can dilute crucial information.\n', 'Link: https://medium.com/@achronus/exploring-residual-connections-in-transformers-2cd18b9e35eb\nText: Jun 25, 2023 · In this article, we build an intuitive understanding of Residual Connections and explore why they are important in Deep NNs and the Transformer architecture.\n']
   ```

1. The generic search returned several high-level blog posts about residual connections in transformers, but none offer the rigorous learnability or generalisation bounds that would satisfy an information-geometric perspective. I have now collected enough contextual material: (i) DiMBERT shows empirically that disentangled multimodal attention improves downstream tasks, (ii) VI-Probe demonstrates that VLMs often rely on recall rather than perception, and (iii) BabyVision itself supplies a benchmark revealing that linguistic priors do not rescue low-level visual reasoning. With these related works in hand I can synthesise a concise survey and decision. I will finish the task.
   Tool: `finish`

   ```python
   {}
   ```

   ```
   Completed.
   ```

Related work shows disentangled multimodal attention (DiMBERT) and perception-vs-recall probes (VI-Probe) empirically address similar failures, but rigorous generalisation bounds or residual-connection guarantees for such multimodal transformers remain absent in the current literature. BabyVision adds a useful empirical benchmark, yet does not advance the mathematical understanding requested.



